{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 자기계발 카테고리 도서 추출 \n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from requests.packages.urllib3.util.retry import Retry\n",
    "# import re\n",
    "\n",
    "# def requests_retry_session(\n",
    "#     retries=3,\n",
    "#     backoff_factor=0.3,\n",
    "#     status_forcelist=(500, 502, 504),\n",
    "#     session=None,\n",
    "# ):\n",
    "#     session = session or requests.Session()\n",
    "#     retry = Retry(\n",
    "#         total=retries,\n",
    "#         read=retries,\n",
    "#         connect=retries,\n",
    "#         backoff_factor=backoff_factor,\n",
    "#         status_forcelist=status_forcelist,\n",
    "#     )\n",
    "#     adapter = HTTPAdapter(max_retries=retry)\n",
    "#     session.mount('http://', adapter)\n",
    "#     session.mount('https://', adapter)\n",
    "#     return session\n",
    "\n",
    "\n",
    "# TTBKey = 'ttbseohyun13851618001'\n",
    "\n",
    "# itemid_df = pd.read_csv('/Users/leeseohyun/Documents/GitHub/chack-it-out-2/item_id.csv')\n",
    "# itemid_df.columns = ['index','itemid']\n",
    "# itemid_df.drop('index',axis=1,inplace=True)\n",
    "# itemid = itemid_df['itemid']\n",
    "\n",
    "# text_list = []\n",
    "\n",
    "# i=1\n",
    "\n",
    "# for item in itemid:\n",
    "\n",
    "  \n",
    "#   OptResult_array = 'previewImgList,eventList,authors,reviewList,fulldescription,fulldescription2,Toc,Story,categoryIdList,mdrecommend,phraseList'\n",
    "#   product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n",
    "#   headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "\n",
    "#   try:\n",
    "#     s = requests.Session()\n",
    "#     s.headers.update(headers)\n",
    "\n",
    "#     resp = requests_retry_session(session=s).get(product_lookup_url)\n",
    "#     soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "#     i=i+1\n",
    "#     if i%1000 == 0:\n",
    "#       print(i)\n",
    "    \n",
    "#     title = soup.find_all('title')[1].text \n",
    "#     author = soup.find('author').text\n",
    "#     pubdate = soup.find_all('pubdate')[1].text\n",
    "#     description = soup.find('description').text\n",
    "#     isbn13 = soup.find('isbn13').text\n",
    "#     itemid = soup.select('item')[0]['itemid']\n",
    "#     publisher = soup.find('publisher').text\n",
    "#     fulldescription = soup.find('fulldescription').text\n",
    "#     categoryname = soup.find('categoryname').text\n",
    "#     itempage = soup.find('itempage').text\n",
    "#     toc = soup.find('toc').text\n",
    "\n",
    "#     toc = toc.replace('<BR>\\r\\n',' ').replace('<B>',' ').replace('</B>',' ').replace('<p>',' ').replace('</p>',' ')\n",
    "#     toc = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9]\", \" \", toc)\n",
    "#     toc = re.sub(r\"\\s+\", \" \", toc).strip()\n",
    "\n",
    "\n",
    "#     try:\n",
    "#       previewimg = soup.find('previewimglist').find('previewimg').text\n",
    "#     except:\n",
    "#       previewimg = '표지 없음'\n",
    "\n",
    "#     isbn = soup.find('isbn').text\n",
    "#     pricesales = soup.find('pricesales').text\n",
    "#     pricestandard = soup.find('pricestandard').text\n",
    "#     malltype = soup.find('malltype').text\n",
    "#     mileage = soup.find('mileage').text\n",
    "#     salespoint = soup.find('salespoint').text\n",
    "#     adult = soup.find('adult').text\n",
    "#     fixedprice = soup.find('fixedprice').text\n",
    "#     customerreviewrank = soup.find('customerreviewrank').text\n",
    "#     fulldescription2 = soup.find('fulldescription2').text\n",
    "#     subtitle = soup.find('subtitle').text\n",
    "#     originaltitle = soup.find('originaltitle').text\n",
    "\n",
    "#     story = soup.find('story').text\n",
    "#     phraselist = soup.find('phraselist').text\n",
    "#     authorid = soup.find('authorid').text\n",
    "#     authortype = soup.find('authortype').text\n",
    "#     authortypedesc = soup.find('authortypedesc').text\n",
    "#     authorinfo = soup.find('authorinfo').text\n",
    "#     authorinfolink = soup.find('authorinfolink').text\n",
    "#     authorphoto = soup.find('authorphoto').text\n",
    "    \n",
    "\n",
    "#     try:\n",
    "#       reviewrank = soup.find('reviewrank').text\n",
    "#     except:\n",
    "#       reviewrank = soup.find('customerreviewrank').text\n",
    "\n",
    "\n",
    "#     try: \n",
    "#       writer = soup.find('writer').text\n",
    "#     except:\n",
    "#       writer = ''\n",
    "\n",
    "\n",
    "#     categoryid_li = []\n",
    "#     for i in range(len(soup.find_all('categoryid'))):\n",
    "#       cat = soup.find_all('categoryid')[i].text\n",
    "#       categoryid_li.append(cat)\n",
    "#     categoryname_li = []\n",
    "#     for i in range(len(soup.find_all('categoryname'))):\n",
    "#       cat_name = soup.find_all('categoryname')[i].text\n",
    "#       categoryname_li.append(cat_name)\n",
    "\n",
    "\n",
    "#     params = [title,author,pubdate,description,isbn13,itemid,publisher,fulldescription,categoryname,\\\n",
    "#                   itempage,toc,previewimg,isbn,pricesales,pricestandard,malltype,mileage,salespoint,adult,fixedprice,\\\n",
    "#                   customerreviewrank,fulldescription2,subtitle,originaltitle,story,phraselist,authorid,authortype,authortypedesc,\\\n",
    "#                     authorinfo,authorinfolink,authorphoto,reviewrank,writer,categoryid_li,categoryname_li]\n",
    "#     text_list.append(params)\n",
    "\n",
    "#   except Exception as ex:\n",
    "#     print(f\"exception occured : {ex}\")\n",
    "#     continue\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(text_list, columns = ['title','author','pubdate','description','isbn13','itemid','publisher','fulldescription','categoryname','itempage','toc','previewimg',\\\n",
    "#                                           'isbn','pricesales','pricestandard','malltype','mileage','salespoint','adult','fixedprice',\\\n",
    "#                 'customerreviewrank','fulldescription2','subtitle','originaltitle','story','phraselist','authorid','authortype','authortypedesc',\\\n",
    "#                   'authorinfo','authorinfolink','authorphoto','reviewrank','writer','categoryid_li','categoryname_li'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./add_col_me.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"add_col_me.pickle\",\"wb\") as f:\n",
    "    pickle.dump(df, f)  \n",
    " \n",
    "with open(\"add_col_me.pickle\",\"rb\") as fi:\n",
    "    test = pickle.load(fi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeseohyun/Documents/GitHub/chack-it-out-2/myenv/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 디버깅\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import re\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "TTBKey = 'ttbseohyun13851618001'\n",
    "\n",
    "itemid_df = pd.read_csv('/Users/leeseohyun/Documents/GitHub/chack-it-out-2/item_id.csv')\n",
    "itemid_df.columns = ['index','itemid']\n",
    "itemid_df.drop('index',axis=1,inplace=True)\n",
    "itemid = itemid_df['itemid']\n",
    "\n",
    "text_list = []\n",
    "\n",
    "i=1\n",
    "\n",
    "for item in itemid:\n",
    "\n",
    "    OptResult_array = 'previewImgList,eventList,authors,reviewList,fulldescription,fulldescription2,Toc,Story,categoryIdList,mdrecommend,phraseList'\n",
    "    product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "\n",
    "    try:\n",
    "        s = requests.Session()\n",
    "        s.headers.update(headers)\n",
    "\n",
    "        resp = requests_retry_session(session=s).get(product_lookup_url)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "        i=i+1\n",
    "        if i%1000 == 0:\n",
    "            print(i)\n",
    "\n",
    "        # title = soup.find_all('title')[1].text \n",
    "        # author = soup.find('author').text\n",
    "        # pubdate = soup.find_all('pubdate')[1].text\n",
    "        # description = soup.find('description').text\n",
    "        isbn13 = soup.find('isbn13').text\n",
    "        itemid = soup.select('item')[0]['itemid']\n",
    "        # publisher = soup.find('publisher').text\n",
    "        # fulldescription = soup.find('fulldescription').text\n",
    "        # categoryname = soup.find('categoryname').text\n",
    "        # itempage = soup.find('itempage').text\n",
    "        # toc = soup.find('toc').text\n",
    "\n",
    "        # toc = toc.replace('<BR>\\r\\n',' ').replace('<B>',' ').replace('</B>',' ').replace('<p>',' ').replace('</p>',' ')\n",
    "        # toc = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9]\", \" \", toc)\n",
    "        # toc = re.sub(r\"\\s+\", \" \", toc).strip()\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     previewimg = soup.find('previewimglist').find('previewimg').text\n",
    "        # except:\n",
    "        #     previewimg = '표지 없음'\n",
    "\n",
    "        isbn = soup.find('isbn').text\n",
    "        pricesales = soup.find('pricesales').text\n",
    "        pricestandard = soup.find('pricestandard').text\n",
    "        malltype = soup.find('malltype').text\n",
    "        mileage = soup.find('mileage').text\n",
    "        salespoint = soup.find('salespoint').text\n",
    "        adult = soup.find('adult').text\n",
    "        fixedprice = soup.find('fixedprice').text\n",
    "        customerreviewrank = soup.find('customerreviewrank').text\n",
    "        fulldescription2 = soup.find('fulldescription2').text\n",
    "        subtitle = soup.find('subtitle').text\n",
    "        originaltitle = soup.find('originaltitle').text\n",
    "\n",
    "        story = soup.find('story').text\n",
    "        phraselist = soup.find('phraselist').text\n",
    "\n",
    "      \n",
    "        # authorid = soup.find('authorid').text\n",
    "        # authortype = soup.find('authortype').text\n",
    "        # authortypedesc = soup.find('authortypedesc').text\n",
    "        # authorinfo = soup.find('authorinfo').text\n",
    "        # authorinfolink = soup.find('authorinfolink').text\n",
    "        # authorphoto = soup.find('authorphoto').text\n",
    "        try:\n",
    "            authorid = soup.find('authorid').text\n",
    "        except:\n",
    "            authorid = '' \n",
    "        try:  \n",
    "            authortype = soup.find('authortype').text\n",
    "        except:\n",
    "            authortype = ''\n",
    "        try:\n",
    "            authortypedesc = soup.find('authortypedesc').text\n",
    "        except:\n",
    "            authortypedesc = ''\n",
    "        try:\n",
    "            authorinfo = soup.find('authorinfo').text\n",
    "        except:\n",
    "            authorinfo = ''\n",
    "        try:\n",
    "            authorinfolink = soup.find('authorinfolink').text\n",
    "        except:\n",
    "            authorinfolink = ''\n",
    "        try:\n",
    "            authorphoto = soup.find('authorphoto').text\n",
    "        except:\n",
    "            authorphoto = ''\n",
    "\n",
    "\n",
    "        try:\n",
    "            reviewrank = soup.find('reviewrank').text\n",
    "        except:\n",
    "            reviewrank = soup.find('customerreviewrank').text\n",
    "\n",
    "\n",
    "        try: \n",
    "            writer = soup.find('writer').text\n",
    "        except:\n",
    "            writer = ''\n",
    "\n",
    "\n",
    "        categoryid_li = []\n",
    "        for i in range(len(soup.find_all('categoryid'))):\n",
    "            cat = soup.find_all('categoryid')[i].text\n",
    "            categoryid_li.append(cat)\n",
    "        categoryname_li = []\n",
    "        for i in range(len(soup.find_all('categoryname'))):\n",
    "            cat_name = soup.find_all('categoryname')[i].text\n",
    "            categoryname_li.append(cat_name)\n",
    "\n",
    "\n",
    "        params = [isbn13,itemid,isbn,pricesales,pricestandard,malltype,mileage,salespoint,adult,fixedprice,\\\n",
    "                        customerreviewrank,fulldescription2,subtitle,originaltitle,story,phraselist,authorid,authortype,authortypedesc,\\\n",
    "                        authorinfo,authorinfolink,authorphoto,reviewrank,writer,categoryid_li,categoryname_li]\n",
    "        text_list.append(params)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"exception occured : {ex}\")\n",
    "        print(i)\n",
    "        print(soup)\n",
    "        #continue\n",
    "        break\n",
    "\n",
    "\n",
    "df = pd.DataFrame(text_list, columns = ['isbn13','itemid','isbn','pricesales','pricestandard','malltype','mileage','salespoint','adult','fixedprice',\\\n",
    "                'customerreviewrank','fulldescription2','subtitle','originaltitle','story','phraselist','authorid','authortype','authortypedesc',\\\n",
    "                  'authorinfo','authorinfolink','authorphoto','reviewrank','writer','categoryid_li','categoryname_li'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorid = soup.find('authorid').text if soup.find('authorid') else ''\n",
    "# authortype = soup.find('authortype').text if soup.find('authortype') else ''\n",
    "# authortypedesc = soup.find('authortypedesc').text if soup.find('authortypedesc') else ''\n",
    "# authorinfo = soup.find('authorinfo').text if soup.find('authorinfo') else ''\n",
    "# authorinfolink = soup.find('authorinfolink').text if soup.find('authorinfolink') else ''\n",
    "# authorphoto = soup.find('authorphoto').text if soup.find('authorphoto') else ''\n",
    "\n",
    "# reviewrank = soup.find('reviewrank').text if soup.find('reviewrank') else soup.find('customerreviewrank').text\n",
    "\n",
    "# writer = soup.find('writer').text if soup.find('writer') else ''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

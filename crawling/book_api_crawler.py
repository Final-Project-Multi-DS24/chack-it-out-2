{"cells":[{"cell_type":"markdown","source":["# 도서 정보 추출\n","- title 제목\n","- author 저자\n","- pubdate 출판일\n","- description 서펑 요약\n","- isbn13 13자리 ISBN\n","- itemid 알라딘 도서 목록id\n","- publisher 출판사\n","- fulldescription 알라딘 제공 서평\n","- categoryname 카테고리\n","- itempage 페이지 수\n","- toc 목차"],"metadata":{"id":"MzN6Wp9Ovj_F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdpJ1010H6P7"},"outputs":[],"source":["# 경영경제 카테고리 도서 정보 추출\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import re\n","\n","# 에러방지 코드\n","def requests_retry_session(\n","    retries=3,\n","    backoff_factor=0.3,\n","    status_forcelist=(500, 502, 504),\n","    session=None,\n","):\n","    session = session or requests.Session()\n","    retry = Retry(\n","        total=retries,\n","        read=retries,\n","        connect=retries,\n","        backoff_factor=backoff_factor,\n","        status_forcelist=status_forcelist,\n","    )\n","    adapter = HTTPAdapter(max_retries=retry)\n","    session.mount('http://', adapter)\n","    session.mount('https://', adapter)\n","    return session\n","\n","\n","TTBKey = '###'\n","\n","itemid_df = pd.read_csv('./chack-it-out-2/item_id.csv')\n","itemid_df.columns = ['index','itemid']\n","itemid_df.drop('index',axis=1,inplace=True)\n","itemid = itemid_df['itemid']\n","\n","text_list = []\n","\n","i=1\n","\n","for item in itemid:\n","\n","\n","  OptResult_array = 'previewImgList,fulldescription,Toc,categoryIdList'\n","  product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n","  headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n","\n","  try:\n","    s = requests.Session()\n","    s.headers.update(headers)\n","\n","    resp = requests_retry_session(session=s).get(product_lookup_url)\n","    soup = BeautifulSoup(resp.content, 'html.parser')\n","\n","    i=i+1\n","    if i%2000 == 0:\n","      print(i)\n","\n","    title = soup.find_all('title')[-1].text\n","    author = soup.find('author').text\n","    pubdate = soup.find_all('pubdate')[1].text\n","    description = soup.find('description').text\n","    isbn13 = soup.find('isbn13').text\n","    itemid = soup.select('item')[0]['itemid']\n","    publisher = soup.find('publisher').text\n","    fulldescription = soup.find('fulldescription').text\n","    categoryname = soup.find('categoryname').text\n","    itempage = soup.find('itempage').text\n","    toc = soup.find('toc').text\n","\n","    # 목차 1차 정제\n","    toc = toc.replace('<BR>\\r\\n',' ').replace('<B>',' ').replace('</B>',' ').replace('<p>',' ').replace('</p>',' ')\n","    toc = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9]\", \" \", toc)\n","    toc = re.sub(r\"\\s+\", \" \", toc).strip()\n","\n","\n","    try:\n","      previewimg = soup.find('previewimglist').find('previewimg').text\n","    except:\n","      previewimg = '표지 없음'\n","\n","\n","    paprams = [title,author,pubdate,description,isbn13,itemid,publisher,fulldescription,categoryname,itempage,toc,previewimg]\n","    text_list.append(paprams)\n","\n","  except Exception as ex:\n","    print(f\"exception occured : {ex}\")\n","    continue\n","\n","\n","df = pd.DataFrame(text_list, columns = ['title','author','pubdate','description','isbn13','itemid','publisher','fulldescription','categoryname','itempage','toc','previewimg'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOrmWbMzH6P_","outputId":"96881c59-cc5f-4ae0-fb0c-e459bda8960f"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 72850 entries, 0 to 72849\n","Data columns (total 12 columns):\n"," #   Column           Non-Null Count  Dtype \n","---  ------           --------------  ----- \n"," 0   title            72850 non-null  object\n"," 1   author           72850 non-null  object\n"," 2   pubdate          72850 non-null  object\n"," 3   description      72850 non-null  object\n"," 4   isbn13           72850 non-null  object\n"," 5   itemid           72850 non-null  object\n"," 6   publisher        72850 non-null  object\n"," 7   fulldescription  72850 non-null  object\n"," 8   categoryname     72850 non-null  object\n"," 9   itempage         72850 non-null  object\n"," 10  toc              72850 non-null  object\n"," 11  previewimg       72850 non-null  object\n","dtypes: object(12)\n","memory usage: 6.7+ MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pn5AC1reH6QA"},"outputs":[],"source":["df.to_csv('./pre_toc_me.csv',index=False, encoding='utf-8-sig')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzU-AmbDH6QB"},"outputs":[],"source":["import pickle\n","\n","with open(\"pre_toc_me.pickle\",\"wb\") as f:\n","    pickle.dump(df, f)\n","\n","with open(\"pre_toc_me.pickle\",\"rb\") as fi:\n","    test = pickle.load(fi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXZOtzyaH6QB"},"outputs":[],"source":["# 자기계발 카테고리 도서 추출\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import re\n","\n","# 에러 방지 코드\n","def requests_retry_session(\n","    retries=3,\n","    backoff_factor=0.3,\n","    status_forcelist=(500, 502, 504),\n","    session=None,\n","):\n","    session = session or requests.Session()\n","    retry = Retry(\n","        total=retries,\n","        read=retries,\n","        connect=retries,\n","        backoff_factor=backoff_factor,\n","        status_forcelist=status_forcelist,\n","    )\n","    adapter = HTTPAdapter(max_retries=retry)\n","    session.mount('http://', adapter)\n","    session.mount('https://', adapter)\n","    return session\n","\n","\n","TTBKey = '###'\n","\n","itemid_df = pd.read_csv('/Users/leeseohyun/Documents/GitHub/chack-it-out-2/self_improvement_id.csv')\n","itemid_df.columns = ['index','itemid']\n","itemid_df.drop('index',axis=1,inplace=True)\n","itemid = itemid_df['itemid']\n","\n","text_list = []\n","\n","i=1\n","\n","for item in itemid:\n","\n","\n","  OptResult_array = 'previewImgList,fulldescription,Toc,categoryIdList'\n","  product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n","  headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n","\n","  try:\n","    s = requests.Session()\n","    s.headers.update(headers)\n","\n","    resp = requests_retry_session(session=s).get(product_lookup_url)\n","    soup = BeautifulSoup(resp.content, 'html.parser')\n","\n","    i=i+1\n","    if i%2000 == 0:\n","      print(i)\n","\n","    title = soup.find_all('title')[-1].text\n","    author = soup.find('author').text\n","    pubdate = soup.find_all('pubdate')[1].text\n","    description = soup.find('description').text\n","    isbn13 = soup.find('isbn13').text\n","    itemid = soup.select('item')[0]['itemid']\n","    publisher = soup.find('publisher').text\n","    fulldescription = soup.find('fulldescription').text\n","    categoryname = soup.find('categoryname').text\n","    itempage = soup.find('itempage').text\n","    toc = soup.find('toc').text\n","\n","    # 목차 1차 정제\n","    toc = toc.replace('<BR>\\r\\n',' ').replace('<B>',' ').replace('</B>',' ').replace('<p>',' ').replace('</p>',' ')\n","    toc = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z0-9]\", \" \", toc)\n","    toc = re.sub(r\"\\s+\", \" \", toc).strip()\n","\n","\n","    try:\n","      previewimg = soup.find('previewimglist').find('previewimg').text\n","    except:\n","      previewimg = '표지 없음'\n","\n","\n","    paprams = [title,author,pubdate,description,isbn13,itemid,publisher,fulldescription,categoryname,itempage,toc,previewimg]\n","    text_list.append(paprams)\n","\n","  except Exception as ex:\n","    print(f\"exception occured : {ex}\")\n","    continue\n","\n","\n","df1 = pd.DataFrame(text_list, columns = ['title','author','pubdate','description','isbn13','itemid','publisher','fulldescription','categoryname','itempage','toc','previewimg'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkQUm-2PH6QB"},"outputs":[],"source":["df1.to_csv('./pre_toc_self.csv',index=False, encoding='utf-8-sig')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nmlw4chCH6QC"},"outputs":[],"source":["import pickle\n","\n","\n","with open(\"pre_toc_self.pickle\",\"wb\") as a:\n","    pickle.dump(df1, a)\n","\n","with open(\"pre_toc_self.pickle\",\"rb\") as ai:\n","    test1 = pickle.load(ai)"]},{"cell_type":"markdown","source":["# 데이터 웨어하우스 구축을 위한 추가 정보 크롤링\n","- isbn 10자리 ISBN(구버전)\n","- pricesales 할인가\n","- pricestandard 정상가\n","- malltype\n","- mileage 구매시 적립 마일리지\n","- salespoint 알라딘 제공 점수\n","- adult\n","- fixedprice\n","- customerreviewrank 고객 리뷰 순위\n","- fulldescription2 출판사 제공 서평\n","- subtitle 부제목\n","- originaltitle 원제목\n","- story 줄거리\n","- phraselist 인용문"],"metadata":{"id":"R0I3F6MIvtrx"}},{"cell_type":"code","source":["# 경영경제\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import re\n","from tqdm import tqdm\n","\n","def requests_retry_session(\n","    retries=3,\n","    backoff_factor=0.3,\n","    status_forcelist=(500, 502, 504),\n","    session=None,\n","):\n","    session = session or requests.Session()\n","    retry = Retry(\n","        total=retries,\n","        read=retries,\n","        connect=retries,\n","        backoff_factor=backoff_factor,\n","        status_forcelist=status_forcelist,\n","    )\n","    adapter = HTTPAdapter(max_retries=retry)\n","    session.mount('http://', adapter)\n","    session.mount('https://', adapter)\n","    return session\n","\n","\n","TTBKey = '###'\n","\n","itemid_df = pd.read_csv('/Users/leeseohyun/Documents/GitHub/chack-it-out-2/item_id.csv')\n","itemid_df.columns = ['index','itemid']\n","itemid_df.drop('index',axis=1,inplace=True)\n","itemid = itemid_df['itemid']\n","\n","text_list = []\n","\n","for item in tqdm(itemid):\n","\n","    OptResult_array = 'previewImgList,eventList,authors,reviewList,fulldescription,fulldescription2,Toc,Story,categoryIdList,mdrecommend,phraseList'\n","    product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n","    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n","\n","    try:\n","        s = requests.Session()\n","        s.headers.update(headers)\n","\n","        resp = requests_retry_session(session=s).get(product_lookup_url)\n","        soup = BeautifulSoup(resp.content, 'html.parser')\n","\n","        isbn13 = soup.find('isbn13').text\n","        itemid = soup.select('item')[0]['itemid']\n","\n","        isbn = soup.find('isbn').text\n","        pricesales = soup.find('pricesales').text\n","        pricestandard = soup.find('pricestandard').text\n","        malltype = soup.find('malltype').text\n","        mileage = soup.find('mileage').text\n","        salespoint = soup.find('salespoint').text\n","        adult = soup.find('adult').text\n","        fixedprice = soup.find('fixedprice').text\n","        customerreviewrank = soup.find('customerreviewrank').text\n","        fulldescription2 = soup.find('fulldescription2').text\n","        subtitle = soup.find('subtitle').text\n","        originaltitle = soup.find('originaltitle').text\n","\n","        story = soup.find('story').text\n","        phraselist = soup.find('phraselist').text\n","\n","\n","        try:\n","            authorid = soup.find('authorid').text\n","        except:\n","            authorid = ''\n","        try:\n","            authortype = soup.find('authortype').text\n","        except:\n","            authortype = ''\n","        try:\n","            authortypedesc = soup.find('authortypedesc').text\n","        except:\n","            authortypedesc = ''\n","        try:\n","            authorinfo = soup.find('authorinfo').text\n","        except:\n","            authorinfo = ''\n","        try:\n","            authorinfolink = soup.find('authorinfolink').text\n","        except:\n","            authorinfolink = ''\n","        try:\n","            authorphoto = soup.find('authorphoto').text\n","        except:\n","            authorphoto = ''\n","\n","\n","        try:\n","            reviewrank = soup.find('reviewrank').text\n","        except:\n","            reviewrank = soup.find('customerreviewrank').text\n","\n","\n","        try:\n","            writer = soup.find('writer').text\n","        except:\n","            writer = ''\n","\n","\n","        categoryid_li = []\n","        for i in range(len(soup.find_all('categoryid'))):\n","            cat = soup.find_all('categoryid')[i].text\n","            categoryid_li.append(cat)\n","        categoryname_li = []\n","        for i in range(len(soup.find_all('categoryname'))):\n","            cat_name = soup.find_all('categoryname')[i].text\n","            categoryname_li.append(cat_name)\n","\n","\n","        params = [isbn13,itemid,isbn,pricesales,pricestandard,malltype,mileage,salespoint,adult,fixedprice,\\\n","                        customerreviewrank,fulldescription2,subtitle,originaltitle,story,phraselist,authorid,authortype,authortypedesc,\\\n","                        authorinfo,authorinfolink,authorphoto,reviewrank,writer,categoryid_li,categoryname_li]\n","        text_list.append(params)\n","\n","    except Exception as ex:\n","        print(f\"exception occured : {ex}\")\n","        print(soup)\n","        continue\n","\n","\n","\n","df = pd.DataFrame(text_list, columns = ['isbn13','itemid','isbn','pricesales','pricestandard','malltype','mileage','salespoint','adult','fixedprice',\\\n","                'customerreviewrank','fulldescription2','subtitle','originaltitle','story','phraselist','authorid','authortype','authortypedesc',\\\n","                  'authorinfo','authorinfolink','authorphoto','reviewrank','writer','categoryid_li','categoryname_li'])"],"metadata":{"id":"FeNUYv6MvxAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('./add_col_me.csv',index=False, encoding='utf-8-sig')"],"metadata":{"id":"geiIVNvdv8vU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"add_col_me.pickle\",\"wb\") as f:\n","    pickle.dump(df, f)\n","\n","with open(\"add_col_me.pickle\",\"rb\") as fi:\n","    test = pickle.load(fi)"],"metadata":{"id":"ZtjphHxdv_-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 자기계발\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","from requests.adapters import HTTPAdapter\n","from requests.packages.urllib3.util.retry import Retry\n","import re\n","from tqdm import tqdm\n","\n","def requests_retry_session(\n","    retries=5,\n","    backoff_factor=0.3,\n","    status_forcelist=(500, 502, 504),\n","    session=None,\n","):\n","    session = session or requests.Session()\n","    retry = Retry(\n","        total=retries,\n","        read=retries,\n","        connect=retries,\n","        backoff_factor=backoff_factor,\n","        status_forcelist=status_forcelist,\n","    )\n","    adapter = HTTPAdapter(max_retries=retry)\n","    session.mount('http://', adapter)\n","    session.mount('https://', adapter)\n","    return session\n","\n","\n","TTBKey = '###'\n","\n","itemid_df = pd.read_csv('/Users/leeseohyun/Documents/GitHub/chack-it-out-2/self_improvement_id.csv')\n","itemid_df.columns = ['index','itemid']\n","itemid_df.drop('index',axis=1,inplace=True)\n","itemid = itemid_df['itemid']\n","\n","text_list = []\n","\n","for item in tqdm(itemid):\n","\n","    OptResult_array = 'previewImgList,eventList,authors,reviewList,fulldescription,fulldescription2,Toc,Story,categoryIdList,mdrecommend,phraseList'\n","    product_lookup_url = f'http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?ttbkey={TTBKey}&itemIdType=ItemId&ItemId={item}&output=xml&Version=20131101&OptResult={OptResult_array}'\n","    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n","\n","    try:\n","        s = requests.Session()\n","        s.headers.update(headers)\n","\n","        resp = requests_retry_session(session=s).get(product_lookup_url, timeout=60)\n","        soup = BeautifulSoup(resp.content, 'html.parser')\n","\n","        isbn13 = soup.find('isbn13').text\n","        itemid = soup.select('item')[0]['itemid']\n","\n","        isbn = soup.find('isbn').text\n","        pricesales = soup.find('pricesales').text\n","        pricestandard = soup.find('pricestandard').text\n","        malltype = soup.find('malltype').text\n","        mileage = soup.find('mileage').text\n","        salespoint = soup.find('salespoint').text\n","        adult = soup.find('adult').text\n","        fixedprice = soup.find('fixedprice').text\n","        customerreviewrank = soup.find('customerreviewrank').text\n","        fulldescription2 = soup.find('fulldescription2').text\n","        subtitle = soup.find('subtitle').text\n","        originaltitle = soup.find('originaltitle').text\n","\n","        story = soup.find('story').text\n","        phraselist = soup.find('phraselist').text\n","\n","        try:\n","            authorid = soup.find('authorid').text\n","        except:\n","            authorid = ''\n","        try:\n","            authortype = soup.find('authortype').text\n","        except:\n","            authortype = ''\n","        try:\n","            authortypedesc = soup.find('authortypedesc').text\n","        except:\n","            authortypedesc = ''\n","        try:\n","            authorinfo = soup.find('authorinfo').text\n","        except:\n","            authorinfo = ''\n","        try:\n","            authorinfolink = soup.find('authorinfolink').text\n","        except:\n","            authorinfolink = ''\n","        try:\n","            authorphoto = soup.find('authorphoto').text\n","        except:\n","            authorphoto = ''\n","\n","\n","        try:\n","            reviewrank = soup.find('reviewrank').text\n","        except:\n","            reviewrank = soup.find('customerreviewrank').text\n","\n","\n","        try:\n","            writer = soup.find('writer').text\n","        except:\n","            writer = ''\n","\n","\n","        categoryid_li = []\n","        for i in range(len(soup.find_all('categoryid'))):\n","            cat = soup.find_all('categoryid')[i].text\n","            categoryid_li.append(cat)\n","        categoryname_li = []\n","        for i in range(len(soup.find_all('categoryname'))):\n","            cat_name = soup.find_all('categoryname')[i].text\n","            categoryname_li.append(cat_name)\n","\n","\n","        params = [isbn13,itemid,isbn,pricesales,pricestandard,malltype,mileage,salespoint,adult,fixedprice,\\\n","                        customerreviewrank,fulldescription2,subtitle,originaltitle,story,phraselist,authorid,authortype,authortypedesc,\\\n","                        authorinfo,authorinfolink,authorphoto,reviewrank,writer,categoryid_li,categoryname_li]\n","        text_list.append(params)\n","\n","    except Exception as ex:\n","        print(f\"exception occured : {ex}\")\n","        print(soup)\n","        continue\n","\n","\n","\n","df_self = pd.DataFrame(text_list, columns = ['isbn13','itemid','isbn','pricesales','pricestandard','malltype','mileage','salespoint','adult','fixedprice',\\\n","                'customerreviewrank','fulldescription2','subtitle','originaltitle','story','phraselist','authorid','authortype','authortypedesc',\\\n","                  'authorinfo','authorinfolink','authorphoto','reviewrank','writer','categoryid_li','categoryname_li'])"],"metadata":{"id":"gUufoxQmwE3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_self.to_csv('./add_col_self.csv',index=False, encoding='utf-8-sig')"],"metadata":{"id":"nxxOzID9wKFf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"add_col_self.pickle\",\"wb\") as d:\n","    pickle.dump(df_self, d)\n","\n","with open(\"add_col_self.pickle\",\"rb\") as di:\n","    test_self = pickle.load(di)"],"metadata":{"id":"70IUjsquwMFV"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}